{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Student Name: Shireen Hassann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, will quantifying the similarity between pairs of words of a dataset using different methods with the word co-occurrence in the Brown corpus and synset structure of WordNet. \n",
    "Firstly, will preprocess the dataset to filter out the rare and ambiguous words. \n",
    "Secondly, will calculate the similarity scores for pairs of words in the filtered dateset using Lin similarity, NPMI and LSA. \n",
    "Lastly,  will quantify how well these methods work by comparing to a human annotated gold-standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: For this homework we will be comparing our methods against a popular dataset of word similarities called <a href=\"http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\">Similarity-353</a>. You need to first obtain this dataset, which is available on LMS. The file we will be using is called *set1.tab*. Make sure you save this in the same folder as the notebook.  Except for the header (which should be stripped out), the file is tab formated with the first two columns corresponding to two words, and the third column representing a human-annotated similarity between the two words. <b>You should ignore the subsequent columns</b>.\n",
    "\n",
    "Here shows the first six lines of the file:\n",
    "\n",
    "```\n",
    "Word 1\tWord 2\tHuman (mean)\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t\n",
    "love\tsex\t6.77\t9\t6\t8\t8\t7\t8\t8\t4\t7\t2\t6\t7\t8\t\n",
    "tiger\tcat\t7.35\t9\t7\t8\t7\t8\t9\t8.5\t5\t6\t9\t7\t5\t7\t\n",
    "tiger\ttiger\t10.00\t10\t10\t10\t10\t10\t10\t10\t10\t10\t10\t10\t10\t10\t\n",
    "book\tpaper\t7.46\t8\t8\t7\t7\t8\t9\t7\t6\t7\t8\t9\t4\t9\t\n",
    "computer\tkeyboard\t7.62\t8\t7\t9\t9\t8\t8\t7\t7\t6\t8\t10\t3\t9\t\n",
    "```\n",
    "    \n",
    "You should load this file into a Python dictionary (NOTE: in Python, tuples of strings, i.e. (\"tiger\",\"cat\") can serve as the keys of a dictionary to map to their human-annotated similarity). This dataset contains many rare words: we need to filter this dataset in order for it to be better suited to the resources we will use in this assignment. So your first goal is to filter this dataset to generate a smaller test set where you will evaluate your word similarity methods.\n",
    "\n",
    "The first filtering is based on document frequencies in the Brown corpus, in order to remove rare words. In this task, we will be treating the paragraphs of the Brown corpus as our \"documents\". You can iterate over them by using the `paras` method of the corpus reader. You should remove tokens that are not alphabetic. Tokens should be lower-cased and lemmatized. Now calculate document frequencies for each word type, and use this to remove from your word similarity data any word pairs where at least one of the two words has a document frequency of less than 8 in this corpus.\n",
    "\n",
    "For this part, store all the word pair and similarity mappings in your filtered test set in a dictionary called *filtered_gold_standard*. \n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/shireenhassan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shireenhassan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "{('love', 'sex'): 6.77, ('tiger', 'cat'): 7.35, ('tiger', 'tiger'): 10.0, ('book', 'paper'): 7.46, ('plane', 'car'): 5.77, ('train', 'car'): 6.31, ('telephone', 'communication'): 7.5, ('television', 'radio'): 6.77, ('drug', 'abuse'): 6.85, ('bread', 'butter'): 6.19, ('doctor', 'nurse'): 7.0, ('professor', 'doctor'): 6.62, ('student', 'professor'): 6.81, ('smart', 'student'): 4.62, ('smart', 'stupid'): 5.81, ('company', 'stock'): 7.08, ('stock', 'market'): 8.08, ('stock', 'phone'): 1.62, ('stock', 'egg'): 1.81, ('stock', 'live'): 3.73, ('stock', 'life'): 0.92, ('book', 'library'): 7.46, ('bank', 'money'): 8.12, ('wood', 'forest'): 7.73, ('money', 'cash'): 9.08, ('king', 'queen'): 8.58, ('bishop', 'rabbi'): 6.69, ('holy', 'sex'): 1.62, ('football', 'basketball'): 6.81, ('football', 'tennis'): 6.63, ('tennis', 'racket'): 7.56, ('law', 'lawyer'): 8.38, ('movie', 'star'): 7.38, ('movie', 'critic'): 6.73, ('movie', 'theater'): 7.92, ('space', 'chemistry'): 4.88, ('alcohol', 'chemistry'): 5.54, ('drink', 'car'): 3.04, ('drink', 'ear'): 1.31, ('drink', 'mouth'): 5.96, ('drink', 'eat'): 6.87, ('baby', 'mother'): 7.85, ('drink', 'mother'): 2.65, ('car', 'automobile'): 8.94, ('journey', 'voyage'): 9.29, ('coast', 'shore'): 9.1, ('furnace', 'stove'): 8.79, ('food', 'fruit'): 7.52, ('tool', 'implement'): 6.46, ('brother', 'monk'): 6.27, ('journey', 'car'): 5.85, ('coast', 'hill'): 4.38, ('forest', 'graveyard'): 1.85, ('monk', 'slave'): 0.92, ('coast', 'forest'): 3.15, ('chord', 'smile'): 0.54, ('noon', 'string'): 0.54, ('money', 'dollar'): 8.42, ('money', 'currency'): 9.04, ('money', 'wealth'): 8.27, ('money', 'property'): 7.57, ('money', 'possession'): 7.29, ('money', 'bank'): 8.5, ('money', 'deposit'): 7.73, ('money', 'operation'): 3.31, ('tiger', 'animal'): 7.0, ('tiger', 'organism'): 4.77, ('tiger', 'zoo'): 5.87, ('psychology', 'anxiety'): 7.0, ('psychology', 'fear'): 6.85, ('psychology', 'depression'): 7.42, ('psychology', 'doctor'): 6.42, ('psychology', 'mind'): 7.69, ('psychology', 'health'): 7.23, ('psychology', 'science'): 6.71, ('psychology', 'discipline'): 5.58, ('planet', 'star'): 8.45, ('planet', 'moon'): 8.08, ('planet', 'sun'): 8.02, ('planet', 'galaxy'): 8.11, ('planet', 'space'): 7.92, ('precedent', 'example'): 5.85, ('precedent', 'information'): 3.85, ('precedent', 'law'): 6.65, ('precedent', 'collection'): 2.5, ('precedent', 'group'): 1.77, ('cup', 'coffee'): 6.58, ('cup', 'article'): 2.4, ('cup', 'object'): 3.69, ('cup', 'entity'): 2.15, ('cup', 'drink'): 7.25, ('cup', 'food'): 5.0, ('cup', 'substance'): 1.92, ('cup', 'liquid'): 5.9}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# filtered_gold_standard stores the word pairs and their human-annotated similarity in your filtered test set\n",
    "filtered_gold_standard = {}\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "file = 'set1.tab'\n",
    "\n",
    "with open(file) as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        list = line.split('\\t')\n",
    "        key = (list[0], list[1])\n",
    "        value = float(list[2])\n",
    "        filtered_gold_standard[key] = value        \n",
    "\n",
    "#List of WordType  \n",
    "doc_brown = brown.paras()\n",
    "wordType = []\n",
    "for i, doc in enumerate(doc_brown):\n",
    "    tempSet = set()\n",
    "    for j, sentence in enumerate(doc):\n",
    "        for k, word in enumerate(sentence):\n",
    "            if(word.isalpha()):\n",
    "                result = lemmatizer.lemmatize(word.lower())\n",
    "                tempSet.add(result)        \n",
    "    wordType.append(tempSet)\n",
    "\n",
    "#Filteing is based on document frequencies \n",
    "for key, value in filtered_gold_standard.items():\n",
    "    sumOne = 0\n",
    "    sumTwo = 0\n",
    "    for word in wordType:\n",
    "        if key[0] in word: \n",
    "            sumOne += 1\n",
    "        if key[1] in word: \n",
    "            sumTwo += 1\n",
    "    if sumOne < 8 or sumTwo < 8:\n",
    "        filtered_gold_standard[key] = False   \n",
    "filtered_gold_standard = {key:value for key, value in filtered_gold_standard.items() if value != False}\n",
    "\n",
    "print(len(filtered_gold_standard))\n",
    "print(filtered_gold_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(filtered_gold_standard) > 50 and len(filtered_gold_standard) < 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filtered_gold_standard[('love', 'sex')] == 6.77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Here, you apply the second filtering. The second filtering is based on words with highly ambiguous senses and involves using the NLTK interface to WordNet. Here, you should remove any words which do not have a *single primary sense*. We define single primary sense here as either a) having only one sense (i.e. only one synset), or b) where the count (as provided by the WordNet `count()` method for the lemmas associated with a synset) of the most common sense is at least 4 times larger than the next most common sense. Note that a synset can be associated with multiple lemmas. You should only consider the count of your lemma. Also, you should remove any words where the primary sense is not a noun (this information is also in the synset). Store the synset corresponding to this primary sense in a dictionary for use in the next section. Given this definition, remove the word pairs from the test set where at least one of the words does not meet the above criteria.\n",
    "\n",
    "When you have applied the two filtering steps, you should store all the word pair and similarity mappings in your filtered test set in a dictionary called *final_gold_standard*. \n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "{('bread', 'butter'): 6.19, ('professor', 'doctor'): 6.62, ('student', 'professor'): 6.81, ('stock', 'egg'): 1.81, ('money', 'cash'): 9.08, ('king', 'queen'): 8.58, ('bishop', 'rabbi'): 6.69, ('football', 'basketball'): 6.81, ('football', 'tennis'): 6.63, ('alcohol', 'chemistry'): 5.54, ('baby', 'mother'): 7.85, ('car', 'automobile'): 8.94, ('journey', 'voyage'): 9.29, ('coast', 'shore'): 9.1, ('furnace', 'stove'): 8.79, ('brother', 'monk'): 6.27, ('journey', 'car'): 5.85, ('coast', 'hill'): 4.38, ('forest', 'graveyard'): 1.85, ('monk', 'slave'): 0.92, ('coast', 'forest'): 3.15, ('psychology', 'doctor'): 6.42, ('psychology', 'mind'): 7.69, ('psychology', 'health'): 7.23, ('psychology', 'science'): 6.71, ('planet', 'moon'): 8.08, ('planet', 'galaxy'): 8.11}\n"
     ]
    }
   ],
   "source": [
    "# final_gold_standard stores the word pairs and their human-annotated similarity in your final filtered test set\n",
    "final_gold_standard = {}\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "wordSynset = {}\n",
    "def countLemmas(word):\n",
    "    list_wordSynset = []\n",
    "    for i in wordnet.synsets(word):\n",
    "        for j in i.lemmas():\n",
    "            if j.name() == word:\n",
    "                list_wordSynset.append((i.name(), j.count()))\n",
    "    list_wordSynset = sorted(list_wordSynset, key=lambda x:x[1], reverse=True)\n",
    "    return list_wordSynset\n",
    "\n",
    "def checkSynset(word):\n",
    "    lemma_list = countLemmas(word)\n",
    "    if len(lemma_list) == 1 and wordnet.synsets(word)[0].pos() == 'n':\n",
    "        wordSynset[word] = wordnet.synsets(word)[0].name()\n",
    "        return True\n",
    "    if lemma_list[0][1] >= 4*lemma_list[1][1]:\n",
    "        name = lemma_list[0][0]\n",
    "        pos = name.split('.')[1]\n",
    "        if pos == 'n':\n",
    "            wordSynset[word] = name\n",
    "            return True    \n",
    "    return False\n",
    "\n",
    "#Filtering is based on words with highly ambiguous senses\n",
    "for key, value in filtered_gold_standard.items():\n",
    "    if checkSynset(key[0]) and checkSynset(key[1]):\n",
    "        continue\n",
    "    else:\n",
    "        filtered_gold_standard[key] = False\n",
    "final_gold_standard = {key:value for key, value in filtered_gold_standard.items() if value != False}\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(len(final_gold_standard))\n",
    "print(final_gold_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(final_gold_standard) > 10 and len(final_gold_standard) < 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(final_gold_standard[('professor', 'doctor')] == 6.62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word similiarity scores with Lin similarity, NPMI and LSA (3 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now you will create several dictionaries with similarity scores for pairs of words in your test set derived using the techniques discussed in class. The first of these is the Lin similarity for your word pairs using the information content of the Brown corpus, which you should calculate using the primary sense for each word derived above. You can use the built-in method included in the NLTK interface, you don't have to implement your own. \n",
    "\n",
    "When you're done, you should store the word pair and similarity mappings in a dictionary called *lin_similarities*.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /Users/shireenhassan/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('bread', 'butter'): 0.711420490146294, ('professor', 'doctor'): 0.7036526610448273, ('student', 'professor'): 0.26208607023317687, ('stock', 'egg'): -0.0, ('money', 'cash'): 0.7888839126424345, ('king', 'queen'): 0.25872135992145145, ('bishop', 'rabbi'): 0.6655650900427844, ('football', 'basketball'): 0.7536025025710653, ('football', 'tennis'): 0.7699955045932811, ('alcohol', 'chemistry'): 0.062235427146896456, ('baby', 'mother'): 0.6315913189894092, ('car', 'automobile'): 1.0, ('journey', 'voyage'): 0.6969176573027711, ('coast', 'shore'): 0.9632173804623256, ('furnace', 'stove'): 0.22813808925013807, ('brother', 'monk'): 0.24862817480738675, ('journey', 'car'): -0.0, ('coast', 'hill'): 0.5991131628821826, ('forest', 'graveyard'): -0.0, ('monk', 'slave'): 0.2543108201944307, ('coast', 'forest'): -0.0, ('psychology', 'doctor'): -0.0, ('psychology', 'mind'): 0.304017384194818, ('psychology', 'health'): 0.06004979886905243, ('psychology', 'science'): 0.8474590505736942, ('planet', 'moon'): 0.14532306834462655, ('planet', 'galaxy'): -0.0}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "# lin_similarities stores the word pair and Lin similarity mappings\n",
    "lin_similarities = {}\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "for key, value in final_gold_standard.items():\n",
    "    firstWord = wordnet.synset(wordSynset[key[0]])\n",
    "    secondWord = wordnet.synset(wordSynset[key[1]])\n",
    "    lin_similarities[key] = firstWord.lin_similarity(secondWord,brown_ic)\n",
    "    \n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(lin_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(lin_similarities[('professor', 'doctor')] > 0.5 and lin_similarities[('professor', 'doctor')] < 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Next, you will calculate Normalized PMI (NPMI) for your word pairs using word frequency derived from the Brown.\n",
    "\n",
    "PMI is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "PMI = \\log_2\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x,y) = \\frac{\\text{Number of paragraphs with the co-occurrence of x and y}}{\\sum_i \\text{Number of word types in paragraph}_i}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x) = \\frac{\\text{Number of paragraphs with the occurrence of x}}{\\sum_i \\text{Number of word types in paragraph}_i}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "p(y) = \\frac{\\text{Number of paragraphs with the occurrence of y}}{\\sum_i \\text{Number of word types in paragraph}_i}\n",
    "\\end{equation*}\n",
    "\n",
    "with the sum over $i$ ranging over all paragraphs. Note that there are other ways PMI could be formulated.\n",
    "\n",
    "NPMI is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "NPMI = \\frac{PMI}{-\\log_2(p(x,y))} = \\frac{\\log_2(p(x)p(y))}{\\log_2(p(x,y))} - 1\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, when there is no co-occurrence, NPMI is -1. NPMI is normalized between [-1, +1].\n",
    "\n",
    "You should use the same set up as you did to calculate document frequency above: paragraphs as documents, lemmatized, lower-cased, and with term frequency information removed by conversion to Python sets. You need to use the basic method for calculating PMI introduced in class (and also in the reading) which is appropriate for any possible definition of co-occurrence (here, there is co-occurrence when a word pair appears in the same paragraph), but you should only calculate PMI for the words in your test set. You must avoid building the entire co-occurrence matrix, instead you should keeping track of the sums you need for the probabilities as you go along. \n",
    "\n",
    "When you have calculated NPMI for all the pairs, you should store the word pair and NPMI-similarity mappings in a dictionary called *NPMI_similarities*.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('bread', 'butter'): 0.6538151518893889, ('professor', 'doctor'): -1, ('student', 'professor'): 0.5365071315202603, ('stock', 'egg'): 0.3751921743542559, ('money', 'cash'): 0.44845734833722983, ('king', 'queen'): 0.41920030980112544, ('bishop', 'rabbi'): -1, ('football', 'basketball'): 0.7167622049649203, ('football', 'tennis'): -1, ('alcohol', 'chemistry'): 0.6253212412399309, ('baby', 'mother'): 0.5164247675130536, ('car', 'automobile'): 0.5440375908153885, ('journey', 'voyage'): -1, ('coast', 'shore'): 0.5910001797885329, ('furnace', 'stove'): -1, ('brother', 'monk'): 0.4309699607308475, ('journey', 'car'): -1, ('coast', 'hill'): 0.34402837373003714, ('forest', 'graveyard'): -1, ('monk', 'slave'): -1, ('coast', 'forest'): 0.4626209010620377, ('psychology', 'doctor'): 0.46517044587124756, ('psychology', 'mind'): 0.44789746893872007, ('psychology', 'health'): -1, ('psychology', 'science'): 0.5916853413144052, ('planet', 'moon'): 0.6587081059716964, ('planet', 'galaxy'): -1}\n"
     ]
    }
   ],
   "source": [
    "# NPMI_similarities stores the word pair and NPMI similarity mappings\n",
    "NPMI_similarities = {}\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "from math import log2\n",
    "probIndividual = {}\n",
    "probCooccurrence   = {}\n",
    "sumOfWordType = 0\n",
    "\n",
    "for i in wordType:\n",
    "    for j in i: \n",
    "        sumOfWordType = sumOfWordType +1\n",
    "\n",
    "for key, value in wordSynset.items():\n",
    "    probIndividual[key] = 0\n",
    "    for i in wordType:\n",
    "        if key in i:\n",
    "            probIndividual[key] += 1\n",
    "    probIndividual[key] = probIndividual[key] / sumOfWordType\n",
    "    \n",
    "for key, value in final_gold_standard.items():\n",
    "    probCooccurrence [key] = 0\n",
    "    for i in wordType:\n",
    "        if key[0] in i and key[1] in i:\n",
    "            probCooccurrence [key] += 1\n",
    "    probCooccurrence [key] = probCooccurrence [key] / sumOfWordType\n",
    "\n",
    "#when there is no co-occurrence, NPMI is -1    \n",
    "for key, value in final_gold_standard.items():\n",
    "    if probCooccurrence [key] <= 0:\n",
    "        NPMI_similarities[key] = -1  \n",
    "    else:  \n",
    "        NPMI_similarities[key] = (log2(probIndividual[key[0]] * probIndividual[key[1]]))/(log2(probCooccurrence [key]))-1\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "\n",
    "print(NPMI_similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(NPMI_similarities[('professor', 'doctor')] == -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** As PMI matrix is very sparse and can be approximated well by a dense representation via singular value decomposition (SVD), you will derive similarity scores using the Latent Semantic Analysis (LSA) method, i.e. apply SVD and truncate to get a dense vector representation of a word type and then calculate cosine similarity between the two vectors for each word pair. You can use the Distributed Semantics notebook as a starting point, but note that since you are interested here in word semantics, you will be constructing a matrix where the (non-sparse) rows correspond to words in the vocabulary, and the (sparse) columns correspond to the texts where they appear (this is the opposite of the notebook). Again, use the Brown corpus, in the same format as with PMI and document frequency. After you have a matrix in the correct format, use `truncatedSVD` in `sklearn` to produce dense vectors of length k = 500, and then use cosine similarity to produce similarities for your word pairs. \n",
    "\n",
    "When you are done, you should store the word pair and LSA-similarity mappings in a dictionary called *LSA_similarities*. \n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('bread', 'butter'): 0.2951162988823682, ('professor', 'doctor'): 0.06269017001792164, ('student', 'professor'): 0.2686080127056509, ('stock', 'egg'): 0.12591854232962824, ('money', 'cash'): 0.1537288213670708, ('king', 'queen'): 0.15916084802614966, ('bishop', 'rabbi'): 0.03705881676102094, ('football', 'basketball'): 0.23351843666126051, ('football', 'tennis'): 0.13928676956933891, ('alcohol', 'chemistry'): 0.08256727968871291, ('baby', 'mother'): 0.3190152022014363, ('car', 'automobile'): 0.3391150376533186, ('journey', 'voyage'): 0.1266235650396778, ('coast', 'shore'): 0.4080920153585178, ('furnace', 'stove'): 0.10149253563173559, ('brother', 'monk'): 0.06820177432287158, ('journey', 'car'): -0.00013027645995288068, ('coast', 'hill'): 0.22204958105994993, ('forest', 'graveyard'): 0.06292790189564043, ('monk', 'slave'): -0.038042130943285395, ('coast', 'forest'): 0.11081581358544729, ('psychology', 'doctor'): 0.1798311442495496, ('psychology', 'mind'): 0.11205451947910017, ('psychology', 'health'): 0.08422108190575449, ('psychology', 'science'): 0.2832501773340568, ('planet', 'moon'): 0.43528444483231093, ('planet', 'galaxy'): 0.06990334546466999}\n"
     ]
    }
   ],
   "source": [
    "# LSA_similarities stores the word pair and LSA similarity mappings\n",
    "LSA_similarities = {}\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "brownList_paras = []\n",
    "for i in wordType:\n",
    "    para_dict = {}\n",
    "    for j in i:\n",
    "        para_dict[j]=1\n",
    "    brownList_paras.append(para_dict)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "matrixBrown = vectorizer.fit_transform(brownList_paras)\n",
    "matrixBrown = matrixBrown.transpose()\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "matrixBrown = svd.fit_transform(matrixBrown)\n",
    "\n",
    "for wordPair in final_gold_standard:\n",
    "    wordOne = matrixBrown[vectorizer.feature_names_.index(wordPair[0]),:]\n",
    "    wordTwo = matrixBrown[vectorizer.feature_names_.index(wordPair[1]),:]\n",
    "    LSA_similarities[tuple((wordPair[0],wordPair[1]))] = (np.dot(wordOne, wordTwo) \n",
    "                                                  / np.sqrt(np.dot(wordOne,wordOne) * np.dot(wordTwo,wordTwo)))\n",
    "    \n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(LSA_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(LSA_similarities[('professor', 'doctor')] > 0 and LSA_similarities[('professor', 'doctor')] < 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison with the Gold Standard (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Instructions:** Finally, you should compare all the similarities you've created to the gold standard you loaded and filtered in the first step. For this, you can use the Pearson correlation co-efficient (`pearsonr`), which is included in scipy (`scipy.stats`). Be careful converting your dictionaries to lists for this purpose, the data for the two datasets needs to be in the same order for correct comparison using correlation. Write a general function, then apply it to each of the similarity score dictionaries.\n",
    "\n",
    "When you are done, you should put the result in a dictionary called *pearson_correlations* (use the keys: 'lin', 'NPMI', 'LSA').  \n",
    "\n",
    "<b>Hint:</b> All of the methods used here should be markedly above 0, but also far from 1 (perfect correlation); if you're not getting reasonable results, go back and check your code for bugs!  \n",
    "\n",
    "(1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lin': 0.44906965748915684, 'NPMI': 0.1268490481420886, 'LSA': 0.4353158562504137}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# pearson_correlations stores the pearson correlations with the gold standard of 'lin', 'NPMI', 'LSA'\n",
    "pearson_correlations = {}\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "def getCorrelation(data,similarity):\n",
    "    listOne = []\n",
    "    listTwo = []\n",
    "    for key, value in data.items():\n",
    "        listOne.append(value)\n",
    "    for key, value in similarity.items():\n",
    "        listTwo.append(value)    \n",
    "    result = pearsonr(listOne, listTwo)\n",
    "    return result\n",
    "\n",
    "lin_correlations= getCorrelation(final_gold_standard,lin_similarities)\n",
    "pearson_correlations['lin'] = lin_correlations[0]\n",
    "\n",
    "NPMI_correlations = getCorrelation(final_gold_standard,NPMI_similarities)\n",
    "pearson_correlations['NPMI'] = NPMI_correlations[0]\n",
    "\n",
    "LSA_correlations= getCorrelation (final_gold_standard,LSA_similarities)\n",
    "pearson_correlations['LSA'] = LSA_correlations[0]\n",
    "   \n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(pearson_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(pearson_correlations['lin'] > 0.4 and pearson_correlations['lin'] < 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A final word\n",
    "\n",
    "Normally, we would not use a corpus as small as the Brown for the purposes of building distributional word vectors. Also, note that filtering our test set to just words we are likely to do well on would typically be considered cheating."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
