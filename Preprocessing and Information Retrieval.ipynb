{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Shireen Hassan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will be using documents from a Wall Street Journal text corpus to create a space efficient inverted index capable of fast TF-IDF query processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework we will be using documents from a Wall Street Journal text corpus. \n",
    "The corpus will be downloaded with the commands below. \n",
    "Each line contains <i>one</i> document which will be tokenize and stem using tools provided by NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download documents from a Wall Street Journal text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "fname = 'wsta_col_20k.gz'\n",
    "my_file = Path(fname)\n",
    "if not my_file.is_file():\n",
    "    url = 'https://trevorcohn.github.io/comp90042/resources/' + fname\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # Save to the current directory\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read raw documents, one document per line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "John Blair & Co. is close to an agreement to sell its TV station advertising representation operation and program production unit to an investor group led by James H. Rosenfield, a former CBS Inc. executive, industry sources said. Industry sources put the value of the proposed acquisition at more than $100 million. John Blair was acquired last year by Reliance Capital Group Inc., which has been divesting itself of John Blair's major assets. John Blair represents about 130 local television stations in the placement of national and other advertising. Mr. Rosenfield stepped down as a senior executive vice president of CBS Broadcasting in December 1985 under a CBS early retirement program. Neither Mr. Rosenfield nor officials of John Blair could be reached for comment. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "raw_docs = []\n",
    "with gzip.open(fname, 'rt') as f:\n",
    "    for raw_doc in f:\n",
    "        raw_docs.append(raw_doc)\n",
    "\n",
    "print(len(raw_docs))\n",
    "print(raw_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preprocessing</b> (1 mark):\n",
    "*tokenize* each document, *stem* and *lowercase* each token using NLTK `word_tokenize` and `PorterStemmer`, and create a *vocabulary* for all the terms (normalized types). \n",
    "Each term will be assigned a unique ID. \n",
    "We are not doing any stop word removal. \n",
    "Vocabulary will be built as a Python *map*, mapping from all the terms $M$ to their term IDs (integers of $[0..M-1]$).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents = 20000\n",
      "Number of unique terms = 103193\n",
      "Number of tokens = 9140697\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# processed_docs stores the list of processed docs\n",
    "processed_docs = []\n",
    "# vocab contains (term, term id) pairs\n",
    "vocab = {}\n",
    "# total_tokens stores the total number of tokens\n",
    "total_tokens = 0\n",
    "\n",
    "# TODO: iterate over docs, tokenize, stem and add to vocab and assign ID if new token\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "for raw_doc in raw_docs:\n",
    "    \n",
    "    # norm_doc stores the normalized tokens of a doc\n",
    "    norm_doc = []\n",
    "    \n",
    "    for token in nltk.word_tokenize(raw_doc):\n",
    "        norm_doc.append(stemmer.stem(token).lower())\n",
    "        total_tokens += 1   \n",
    "    for doc_token in norm_doc:\n",
    "        token_id = len(vocab)\n",
    "        if doc_token not in vocab.keys():\n",
    "            vocab[doc_token] = token_id\n",
    "    \n",
    "    processed_docs.append(norm_doc)   \n",
    "    \n",
    "\n",
    "    \n",
    "print(\"Number of documents = {}\".format(len(processed_docs)))\n",
    "print(\"Number of unique terms = {}\".format(len(vocab)))\n",
    "print(\"Number of tokens = {}\".format(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(processed_docs) == 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(vocab) > 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Build a Python `Counter`</b>: (1 mark)\n",
    "To count the term frequencies of each document. \n",
    "For each document, the counter will map the terms to term frequencies. \n",
    "All the counters (for all documents) will be stored in a list called *doc_term_freqs*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "Counter({'.': 6, 'john': 5, 'blair': 5, 'of': 5, 'to': 3, 'rosenfield': 3, ',': 3, 'a': 3, 'cb': 3, 'the': 3, 'an': 2, 'station': 2, 'advertis': 2, 'and': 2, 'program': 2, 'group': 2, 'by': 2, 'inc.': 2, 'execut': 2, 'industri': 2, 'sourc': 2, 'in': 2, 'mr.': 2, '&': 1, 'co.': 1, 'is': 1, 'close': 1, 'agreement': 1, 'sell': 1, 'it': 1, 'tv': 1, 'represent': 1, 'oper': 1, 'product': 1, 'unit': 1, 'investor': 1, 'led': 1, 'jame': 1, 'h.': 1, 'former': 1, 'said': 1, 'put': 1, 'valu': 1, 'propos': 1, 'acquisit': 1, 'at': 1, 'more': 1, 'than': 1, '$': 1, '100': 1, 'million': 1, 'wa': 1, 'acquir': 1, 'last': 1, 'year': 1, 'relianc': 1, 'capit': 1, 'which': 1, 'ha': 1, 'been': 1, 'divest': 1, 'itself': 1, \"'s\": 1, 'major': 1, 'asset': 1, 'repres': 1, 'about': 1, '130': 1, 'local': 1, 'televis': 1, 'placement': 1, 'nation': 1, 'other': 1, 'step': 1, 'down': 1, 'as': 1, 'senior': 1, 'vice': 1, 'presid': 1, 'broadcast': 1, 'decemb': 1, '1985': 1, 'under': 1, 'earli': 1, 'retir': 1, 'neither': 1, 'nor': 1, 'offici': 1, 'could': 1, 'be': 1, 'reach': 1, 'for': 1, 'comment': 1})\n",
      "Counter({'the': 9, 'of': 4, ',': 4, 'soviet': 3, 'to': 3, 'bank': 3, 'and': 3, 'said': 2, 'it': 2, 'an': 2, 'with': 2, 'institut': 2, 'in': 2, 'is': 2, 'aid': 2, '.': 2, 'banqu': 1, 'de': 1, \"l'union\": 1, 'europeen': 1, 'sign': 1, 'agreement': 1, 'two': 1, 'union': 1, 'that': 1, 'design': 1, 'format': 1, 'joint': 1, 'ventur': 1, 'under': 1, 'new': 1, 'rule': 1, 'french': 1, 'arm': 1, 'state-own': 1, 'credit': 1, 'industri': 1, '&': 1, 'commerci': 1, 'financi': 1, 'group': 1, 'will': 1, 'work': 1, 'state': 1, 'for': 1, 'foreign': 1, 'trade': 1, '``': 1, 'creat': 1, 'a': 1, 'bilater': 1, 'whose': 1, 'aim': 1, 'promot': 1, 'creation': 1, 'function': 1, 'financ': 1, 'mixed-capit': 1, 'compani': 1, 'u.s.s.r': 1, \"''\": 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# doc_term_freqs stores the counters (mapping terms to term frequencies) of all documents\n",
    "doc_term_freqs = []\n",
    "\n",
    "# TODO iterate over document and for each document produce the term frequency map and store in the list\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "for doc in processed_docs:    \n",
    "    doc_term_freqs.append(Counter(doc))\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(len(doc_term_freqs))\n",
    "print(doc_term_freqs[0])\n",
    "print(doc_term_freqs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(doc_term_freqs) == 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(doc_term_freqs[0][\"blair\"] == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(doc_term_freqs[100][\"bank\"] == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inverted Index (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an `InvertedIndex` class using `vocab` and `doc_term_freqs` \n",
    "\n",
    "Our `InvertedIndex` class contains <b>six</b> components:\n",
    "\n",
    "1. The vocabulary `vocab`, which will be used to map query terms to term ids\n",
    "2. The length of each document,  `doc_len`\n",
    "3. `doc_ids` is a list indexed by term IDs. For each term ID, it stores a list of document ids of all documents containing that term\n",
    "4. `doc_term_freqs` is a list indexed by term IDs. For each term ID, it stores a list of document term frequencies $f_{d,t}$ (how often a document $d$ contains the term $t$) of the corresponding documents stored in `doc_ids`\n",
    "5. `doc_freqs` is a list indexed by term IDs. For each term ID, it stores the document frequency $f_t$ indicating the number of documents containing one or more occurrences of term $t$;\n",
    "6. Two integers `total_num_docs` and `max_doc_len` store the total number of documents and the maximum document length\n",
    "\n",
    "These values will be used in the code below. Note that some of these components are for display purposes to verify that the implementation was correctly processing the text collection, but won't be used in TF-IDF scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents = 20000\n",
      "number of terms = 103193\n",
      "longest document length = 10576\n",
      "uncompressed space usage MiB = 58.187\n"
     ]
    }
   ],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "        # this function assumes each integer is stored using 8 bytes\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list) * 8\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list) * 8\n",
    "        return space_usage\n",
    "    \n",
    "\n",
    "invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "# print inverted index stats\n",
    "print(\"documents = {}\".format(invindex.num_docs()))\n",
    "print(\"number of terms = {}\".format(invindex.num_terms()))\n",
    "print(\"longest document length = {}\".format(invindex.max_doc_len))\n",
    "print(\"uncompressed space usage MiB = {:.3f}\".format(invindex.space_in_bytes() / (1024.0 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Use the `InvertedIndex` class</b>:(1 mark)\n",
    "\n",
    "To compute the TF-IDF similarity scores for the documents given a simple query $Q$.\n",
    "\n",
    "Here is a simplified formula for computing TF-IDF similarity scores:\n",
    "\n",
    "\\begin{equation*}\n",
    "Score(Q,d) = \\frac{1}{\\sqrt{|d|}} \\times \\sum_{i=1}^q \\log(1 + f_{d,t}) * \\log( \\frac{N}{f_t} ) \n",
    "\\end{equation*}\n",
    "\n",
    "where $Q$ corresponds to a query containing $q$ query terms, $\\sqrt{|d|}$ corresponds to the length of the document (in words), $f_{d,t}$ corresponds to the frequency of term $t$ in document $d$, $N$ corresponds to the number of documents in the collection, and $f_t$ corresponds to the document frequency of term $t$. All these information are available in the `InvertedIndex` class. Note that the formulation of TF-IDF is a little different to the formula for TF-IDF in the lectures. We have adapted the formulation here to allow for a simpler implementation, e.g., avoiding the need for repeated passes over the dataset. (All manner of variants of TF-IDF exist in practise.)\n",
    "\n",
    "Will implement the `query_tfidf` function. The `query_tfidf` function should take a query and an inverted index and output the top $k$ highest scoring documents. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK  1 DOCID     1084 SCORE 1.210 CONTENT Unemployment in South Korea fell to 3.8% of the labor force last year from \n",
      "RANK  2 DOCID      905 SCORE 1.127 CONTENT Seasonally adjusted industrial production in South Korea increased nearly 2\n",
      "RANK  3 DOCID     5612 SCORE 1.056 CONTENT Consumer prices in South Korea rose 2.4% in April from a year-earlier and 0\n",
      "RANK  4 DOCID     9126 SCORE 0.998 CONTENT Foreign investment in South Korea totaled $278 million in 1987's first quar\n",
      "RANK  5 DOCID    17960 SCORE 0.936 CONTENT Henley Group Inc. said its M.W. Kellogg Co. unit received a contract to des\n",
      "RANK  6 DOCID     1760 SCORE 0.926 CONTENT Consumer prices in South Korea rose 1% in February from a year earlier, the\n",
      "RANK  7 DOCID     4132 SCORE 0.926 CONTENT South Korea's trade deficit with Japan grew to a record $629 million in Apr\n",
      "RANK  8 DOCID    17826 SCORE 0.923 CONTENT South Korea revised its 1986 current-account surplus to $5 billion from the\n",
      "RANK  9 DOCID    15803 SCORE 0.911 CONTENT South Korea's 1986 trade surplus with the U.S. was revised upward to $7.41 \n",
      "RANK 10 DOCID    10664 SCORE 0.889 CONTENT South Korea's economy, aided by brisk exports, grew an inflation-adjusted 1\n"
     ]
    }
   ],
   "source": [
    "from math import log, sqrt\n",
    "\n",
    "# given a query and an index returns a list of the k highest scoring documents as tuples containing <docid,score>\n",
    "def query_tfidf(query, index, k=10):\n",
    "    \n",
    "    # scores stores doc ids and their scores\n",
    "    scores = Counter()\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    for query_term in query:\n",
    "        for idx, docid in enumerate(index.docids(query_term)):\n",
    "            sumCalc = 0\n",
    "            doc_lenght = (sqrt(index.doc_len[docid]))**(-1)\n",
    "            sumCalc += log(1 + index.freqs(query_term)[idx]) * \\\n",
    "                    log(index.num_docs()/index.f_t(query_term))\n",
    "            scores[docid] += doc_lenght * sumCalc \n",
    "            \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    \n",
    "    return scores.most_common(k)\n",
    "\n",
    "\n",
    "# We output some statistics from our index\n",
    "query = \"south korea production\"\n",
    "stemmed_query = nltk.stem.PorterStemmer().stem(query).split()\n",
    "results = query_tfidf(stemmed_query, invindex)\n",
    "for rank, res in enumerate(results):\n",
    "    # e.g RANK 1 DOCID 176 SCORE 0.426 CONTENT South Korea rose 1% in February from a year earlier, the\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],raw_docs[res[0]][:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 1: DOCID\n",
    "assert(results[0][0] > 500 and results[0][0] < 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 1: SCORE\n",
    "assert(results[0][1] > 0.5 and results[0][1] < 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vbyte compression and decompression (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reduce the space usage of the inverted index by compression. We will <i>compress</i> the `doc_ids` and `doc_term_freqs` lists in the inverted index using <b>vbyte</b> compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Implement two methods</b>: (1 mark) \n",
    "\n",
    "To perform vbyte compression and decompression as described in the lecture slides. \n",
    "The method signatures are provided below. \n",
    "\n",
    "- The first method `vbyte_encode(num)` should receive a number as an integer and produces a list of output bytes encoding the number. \n",
    "- The second method `vbyte_decode(input_bytes, idx)` should receive a list of input bytes and an offset into that list where the decompression should start. It returns the decoded number and the number of bytes consumed to decode the number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vbyte_encode(num):\n",
    "\n",
    "    # out_bytes stores a list of output bytes encoding the number\n",
    "    out_bytes = []\n",
    "    \n",
    "    while(num >= 128):\n",
    "        out_bytes.append(num%128)\n",
    "        num = num//128\n",
    "    out_bytes.append(num + 128)  \n",
    " \n",
    "    \n",
    "    return out_bytes\n",
    "\n",
    "\n",
    "def vbyte_decode(input_bytes, idx):\n",
    "    \n",
    "    # x stores the decoded number\n",
    "    x = 0\n",
    "    # consumed stores the number of bytes consumed to decode the number\n",
    "    consumed = 0\n",
    "    s = 0\n",
    "    while(input_bytes[idx]<128):\n",
    "        x = x^(input_bytes[idx]<<s)\n",
    "        s = s + 7\n",
    "        idx+=1\n",
    "        consumed+=1\n",
    "    x = x^((input_bytes[idx]-128)<<s)\n",
    "    consumed = consumed +1    \n",
    "\n",
    "    return x, consumed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a sanity check, we ensure that compression and decompression work correctly:\n",
    "for num in range(0, 123456):\n",
    "    vb = vbyte_encode(num)\n",
    "    dec, decoded_bytes = vbyte_decode(vb, 0)\n",
    "    assert(num == dec)\n",
    "    assert(decoded_bytes == len(vb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Modify the `InvertedIndex` class</b>: (1 mark)\n",
    "\n",
    "To support compression.\n",
    "\n",
    "Implement the compression of the `doc_ids` and `doc_term_freqs` lists using the `vbyte_encode` function implemented earlier. Note that the `doc_ids` have to be gap encoded as described in the lecture slides.  Will use function `decompress_list` to allow decompression of the lists. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents = 20000\n",
      "unique terms = 103193\n",
      "longest document = 10576\n",
      "compressed space usage MiB = 7.818\n"
     ]
    }
   ],
   "source": [
    "def decompress_list(input_bytes, gapped_encoded):\n",
    "    res = []\n",
    "    prev = 0\n",
    "    idx = 0\n",
    "    while idx < len(input_bytes):\n",
    "        dec_num, consumed_bytes = vbyte_decode(input_bytes, idx)\n",
    "        idx += consumed_bytes\n",
    "        num = dec_num + prev\n",
    "        res.append(num)\n",
    "        if gapped_encoded:\n",
    "            prev = num\n",
    "    return res\n",
    "\n",
    "class CompressedInvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "        # TODO NOW WE COMPRESS THE LISTS\n",
    "\n",
    "        output_dtf = []\n",
    "        for idx, term_list in  enumerate(self.doc_term_freqs):\n",
    "            term_compressed_list = list()\n",
    "            for num in term_list:\n",
    "                term_compressed_list.extend(vbyte_encode(num))\n",
    "            output_dtf.append(term_compressed_list)\n",
    "        self.doc_term_freqs = output_dtf\n",
    "        \n",
    "        \n",
    "        output_docid = []\n",
    "        for val, d_id_list in enumerate(self.doc_ids):\n",
    "            term_compressed_list = list()\n",
    "            for i in range(0,len(d_id_list)):\n",
    "                if i == 0:\n",
    "                    term_compressed_list.extend(vbyte_encode(d_id_list[i]))\n",
    "                else:\n",
    "                    diff = (d_id_list[i]) - (d_id_list[i-1])  \n",
    "                    term_compressed_list.extend(vbyte_encode(diff))\n",
    "            output_docid.append(term_compressed_list)\n",
    "        self.doc_ids = output_docid\n",
    "    \n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        # We decompress\n",
    "        return decompress_list(self.doc_ids[term_id], True)\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        # We decompress\n",
    "        return decompress_list(self.doc_term_freqs[term_id], False)\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "        # this function assumes the integers are now bytes\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list)\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list)\n",
    "        return space_usage\n",
    "\n",
    "\n",
    "# We output the same statistics as before to ensure we still store the same data but now use much less space\n",
    "compressed_index = CompressedInvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "print(\"documents = {}\".format(compressed_index.num_docs()))\n",
    "print(\"unique terms = {}\".format(compressed_index.num_terms()))\n",
    "print(\"longest document = {}\".format(compressed_index.max_doc_len))\n",
    "print(\"compressed space usage MiB = {:.3f}\".format(compressed_index.space_in_bytes() / (1024.0 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK  1 DOCID     1084 SCORE 1.210 CONTENT Unemployment in South Korea fell to 3.8% of the labor force last year from \n",
      "RANK  2 DOCID      905 SCORE 1.127 CONTENT Seasonally adjusted industrial production in South Korea increased nearly 2\n",
      "RANK  3 DOCID     5612 SCORE 1.056 CONTENT Consumer prices in South Korea rose 2.4% in April from a year-earlier and 0\n",
      "RANK  4 DOCID     9126 SCORE 0.998 CONTENT Foreign investment in South Korea totaled $278 million in 1987's first quar\n",
      "RANK  5 DOCID    17960 SCORE 0.936 CONTENT Henley Group Inc. said its M.W. Kellogg Co. unit received a contract to des\n",
      "RANK  6 DOCID     1760 SCORE 0.926 CONTENT Consumer prices in South Korea rose 1% in February from a year earlier, the\n",
      "RANK  7 DOCID     4132 SCORE 0.926 CONTENT South Korea's trade deficit with Japan grew to a record $629 million in Apr\n",
      "RANK  8 DOCID    17826 SCORE 0.923 CONTENT South Korea revised its 1986 current-account surplus to $5 billion from the\n",
      "RANK  9 DOCID    15803 SCORE 0.911 CONTENT South Korea's 1986 trade surplus with the U.S. was revised upward to $7.41 \n",
      "RANK 10 DOCID    10664 SCORE 0.889 CONTENT South Korea's economy, aided by brisk exports, grew an inflation-adjusted 1\n"
     ]
    }
   ],
   "source": [
    "# Additionally we want to ensure that the index still returns the same results as before\n",
    "query = \"south korea production\"\n",
    "stemmed_query = nltk.stem.PorterStemmer().stem(query).split()\n",
    "comp_results = query_tfidf(stemmed_query, compressed_index)\n",
    "for rank, res in enumerate(comp_results):\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],raw_docs[res[0]][:75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
