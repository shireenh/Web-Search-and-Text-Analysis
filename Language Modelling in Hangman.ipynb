{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling in Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Student Name: Shireen Hassann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hangman Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=\"https://en.wikipedia.org/wiki/Hangman_(game)\">Hangman game</a> is a simple game whereby one person thinks of a word, which they keep secret from their opponent, who tries to guess the word one character at a time. The game ends when the opponent makes more than a fixed number of incorrect guesses, or they figure out the secret word before then (in which case they *win*). \n",
    "\n",
    "Here's a simple version of the game, and a method allowing interactive play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowing better python 2 & python 3 compatibility \n",
    "from __future__ import print_function \n",
    "\n",
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True, **guesser_args):\n",
    "    \"\"\"\n",
    "        secret_word: a string of lower-case alphabetic characters, i.e., the answer to the game\n",
    "        guesser: a function which guesses the next character at each stage in the game\n",
    "            The function takes a:\n",
    "                mask: what is known of the word, as a string with _ denoting an unknown character\n",
    "                guessed: the set of characters which already been guessed in the game\n",
    "                guesser_args: additional (optional) keyword arguments, i.e., name=value\n",
    "        max_mistakes: limit on length of game, in terms of allowed mistakes\n",
    "        verbose: be chatty vs silent\n",
    "        guesser_args: keyword arguments to pass directly to the guesser function\n",
    "    \"\"\"\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed, **guesser_args)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            if guess in secret_word:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)    \n",
    "    return mistakes\n",
    "\n",
    "def human(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    simple function for manual play\n",
    "    \"\"\"\n",
    "    print('Enter your guess:')\n",
    "    try:\n",
    "        return raw_input().lower().strip() # python 3\n",
    "    except NameError:\n",
    "        return input().lower().strip() # python 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play the game interactively using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hangman('whatever', human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: We will be using the words occurring in the *Brown* corpus for *training* an artificial intelligence guessing algorithm, and for *evaluating* the quality of the method. Note that we are intentionally making the hangman game hard, as the AI will need to cope with test words that it has not seen before, hence it will need to learn generalisable patterns of characters to make reasonable predictions.\n",
    "\n",
    "Your first task is to compute the unique word types occurring in the *Brown* corpus, using `nltk.corpus.Brown`, selecting only words that are entirely comprised of alphabetic characters, and lowercasing the words. Finally, randomly shuffle (`numpy.random.shuffle`) this collection of word types, and split them into disjoint training and testing sets. The test set should contain 1000 word types, and the rest should be in the training set. Your code should print the sizes of the training and test sets.\n",
    "\n",
    "Feel free to test your own Hangman performance using `hangman(numpy.random.choice(test_set), human, 8, True)`. It is surprisingly difficult (and addictive)!\n",
    "\n",
    "(0.5 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of test set is 1000\n",
      "The length of training set is 39234\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "brown_corpus = nltk.corpus.brown.words()\n",
    "\n",
    "word_type = set()\n",
    "for i in brown_corpus:\n",
    "    if i.isalpha():\n",
    "        word_type.add(i.lower())\n",
    "\n",
    "word_type = list(word_type)\n",
    "np.random.shuffle(word_type)\n",
    "test_set = word_type[:1000]\n",
    "training_set = word_type[1000:]\n",
    "print('The length of test set is', len(test_set))\n",
    "print('The length of training set is', len(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hangman(np.random.choice(test_set), human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: To set a baseline, your first *AI* attempt will be a trivial random method. For this you should implement a guessing method, similar to the `human` method above, i.e., using the same input arguments and returning a character. Your method should randomly choose a character from the range `'a'...'z'` after excluding the characters that have already been guessed in the current game (all subsequent AI approaches should also exclude previous guesses). You might want to use `numpy.random.choice` for this purpose.\n",
    "\n",
    "To measure the performance of this (and later) techiques, implement a method that measures the average number of mistakes made by this technique over all the words in the `test_set`. You will want to turn off the printouts for this, using the `verbose=False` option, and increase the cap on the game length to `max_mistakes=26`. Print the average number of mistakes for the random AI, which will become a baseline for the following steps.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average baseline guessing number is 16.722\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def ai(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    function for AI play\n",
    "    \"\"\"\n",
    "    characters = list(string.ascii_lowercase)\n",
    "    guessed = list(guessed)\n",
    "\n",
    "    for i in guessed:\n",
    "        characters.remove(i)\n",
    "    # print('Enter your guess:')\n",
    "    # return raw_input().lower().strip() # python 3\n",
    "    return np.random.choice(characters)\n",
    "\n",
    "mistakes_guess = [hangman(i, ai, 26, False) for i in test_set]\n",
    "#mistakes_guess = hangman(test_set[1], ai, 26, False)\n",
    "baseline = np.average(mistakes_guess)\n",
    "print('The average baseline guessing number is', baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** As your first real AI, you should train a *unigram* model over the training set.  This requires you to find the frequencies of characters over all training words. Using this model, you should write a guess function that returns the character with the highest probability, after aggregating (summing) the probability of each blank character in the secret word. Print the average number of mistakes the unigram method makes over the test set. Remember to exclude already guessed characters, and use the same evaluation method as above, so the results are comparable. (Hint: it should be much lower than for random).\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average unigram model guessing number is 10.478\n"
     ]
    }
   ],
   "source": [
    "def charProbability(training_set):\n",
    "    character_probability = {}\n",
    "    character_frequency = {}\n",
    "   \n",
    "    for i in training_set:\n",
    "        for j in i:\n",
    "            character_frequency[j] = character_frequency.get(j, 0) + 1\n",
    "    total_frequency = np.sum(list(character_frequency.values()))\n",
    "\n",
    "    for k, v in character_frequency.items():\n",
    "        character_probability[k] = v / total_frequency\n",
    "        \n",
    "    return character_probability\n",
    "        \n",
    "def guess(character_list, character_probability):\n",
    "    character = {}\n",
    "    for i in character_list:\n",
    "        character[i] = character_probability.get(i, 0)\n",
    "    sorted_character = sorted(character.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_character[0][0]\n",
    "\n",
    "def unigramAi(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    function for unigram AI play\n",
    "    \"\"\"\n",
    "    character_probability = kwargs.get('character', 0)\n",
    "    characters = list(string.ascii_lowercase)\n",
    "    guessed = list(guessed)\n",
    "    for i in guessed:\n",
    "        characters.remove(i)\n",
    "    char = guess(characters, character_probability)\n",
    "    return char\n",
    "\n",
    "character_probability = charProbability(training_set)\n",
    "kwargs = {'character': character_probability}\n",
    "mistakes_guess = [hangman(i, unigramAi, 26, False, **kwargs) for i in test_set]\n",
    "unigram = np.average(mistakes_guess)\n",
    "print('The average unigram model guessing number is', unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** The length of the secret word is an important clue that we might exploit. Different length words tend to have different distributions over characters, e.g., short words are less likely to have suffixes or prefixes. Your job now is to incorporate this idea by conditioning the unigram model on the length of the secret word, i.e., having *different* unigram models for each length of word. You will need to be a little careful at test time, to be robust to the (unlikely) situation that you encounter a word length that you didn't see in training. Create another AI guessing function using this new model, and print its test performance.   \n",
    "\n",
    "(0.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average wrong guessing number of unigram model incorporating with word length is 10.378\n"
     ]
    }
   ],
   "source": [
    "# store the character probability based on different word length\n",
    "# sample pair {word_lenght: {prbability_of_characters}}\n",
    "corpus_probability = {}\n",
    "train_length_set = set()\n",
    "\n",
    "for i in training_set:\n",
    "    train_length_set.add(len(i))\n",
    "\n",
    "def dynamicCharProbability(training_set, **kwargs):\n",
    "    \"\"\"\n",
    "    **kwargs can be used for passing word length later\n",
    "    \"\"\"\n",
    "    word_length = kwargs.get('length', 0)\n",
    "    character_probability = {}\n",
    "    character_frequency = {}\n",
    "    if not word_length:\n",
    "        for i in training_set:\n",
    "            for j in i:\n",
    "                character_frequency[j] = character_frequency.get(j, 0) + 1\n",
    "        total_frequency = np.sum(list(character_frequency.values()))\n",
    "\n",
    "        for k, v in character_frequency.items():\n",
    "            character_probability[k] = v / total_frequency\n",
    "    else:\n",
    "        if corpus_probability.get(word_length, 0) == 0:\n",
    "            # get the word that has length = word_length\n",
    "            corpus = [i for i in training_set if len(i) == word_length]\n",
    "            for i in corpus:\n",
    "                for j in i:\n",
    "                    character_frequency[j] = character_frequency.get(j, 0) + 1\n",
    "            total_frequency = np.sum(list(character_frequency.values()))\n",
    "\n",
    "            for k, v in character_frequency.items():\n",
    "                character_probability[k] = v / total_frequency\n",
    "            corpus_probability[word_length] = character_probability\n",
    "        else:\n",
    "            character_probability = corpus_probability[word_length]\n",
    "    return character_probability\n",
    "\n",
    "\n",
    "def conditional_unigramAi(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    function for unigram AI play\n",
    "    \"\"\"\n",
    "    character_probability = kwargs.get('character', 0)\n",
    "    characters = list(string.ascii_lowercase)\n",
    "    guessed = list(guessed)\n",
    "    for i in guessed:\n",
    "        characters.remove(i)\n",
    "    char = guess(characters, character_probability)\n",
    "    return char\n",
    "\n",
    "mistakes_guess = []\n",
    "for i in test_set:\n",
    "    mistakes = 0\n",
    "    word_length = len(i)\n",
    "    if word_length not in train_length_set:\n",
    "        character_probability = dynamicCharProbability(training_set)\n",
    "        kwargs = {'character': character_probability}\n",
    "        mistakes = hangman(i, conditional_unigramAi, 26, False, **kwargs) \n",
    "        mistakes_guess.append(mistakes)\n",
    "    else:\n",
    "        kwargs = {'length': word_length}\n",
    "        character_probability = dynamicCharProbability(training_set, **kwargs)\n",
    "        kwargs = {'character': character_probability}\n",
    "        mistakes = hangman(i, conditional_unigramAi, 26, False, **kwargs) \n",
    "        mistakes_guess.append(mistakes)\n",
    "\n",
    "condition_unigram = np.average(mistakes_guess)\n",
    "print('The average wrong guessing number of unigram model incorporating with word length is', condition_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Now for the main challenge, using a *ngram* language model over characters. The order of characters is obviously important, yet this wasn't incorporated in any of the above models. Knowing that the word has the sequence `n _ s s` is a pretty strong clue that the missing character might be `e`. Similarly the distribution over characters that start or end a word are highly biased (e.g., toward common prefixes and suffixes, like *un-*, *-ed* and *-ly*).\n",
    "\n",
    "Your job is to develop a *ngram* language model over characters, train this over the training words (being careful to handle the start of each word properly, e.g., by padding with sentinel symbols.) You should use linear interpolation to smooth between the higher order and lower order models, and you will have to decide how to weight each component. \n",
    "\n",
    "Your guessing AI algorithm should apply your language model to each blank position in the secret word by using as much of the left context as is known. E.g., in `_ e c _ e _ _` we know the full left context for the first blank (context=start of word), we have a context of two characters for the second blank (context=ec), one character for the second last blank (context=e), and no known context for the last one. If we were using a *n=3* order model, we would be able to apply it to the first and second blanks, but would only be able to use the bigram or unigram distributions for the subsequent blanks. As with the unigram model, you should sum over the probability distributions for each blank to find the expected count for each character type, then select the  character with the highest expected count.\n",
    "\n",
    "Implement the ngram method for *n=3,4* and *5* and evaluate each of these three models over the test set. Do you see any improvement over the unigram methods above?\n",
    "\n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the ngram models\n",
    "def getNgramSet(training_set, n):\n",
    "    ngram_dict = {}\n",
    "    for i in training_set:\n",
    "        for j in range(len(i)):\n",
    "            if j < n-1:\n",
    "                ngram_dict['$'*(n-1-j)+i[0:j+1]] = ngram_dict.get('$'*(n-1-j)+i[0:j+1], 0) + 1\n",
    "            else:\n",
    "                ngram_dict[i[j-n+1:j+1]] = ngram_dict.get(i[j-n+1:j+1], 0) + 1\n",
    "                \n",
    "    total_frequency = 0\n",
    "    for k,v in ngram_dict.items():\n",
    "        total_frequency = total_frequency + v\n",
    "    for k,v in ngram_dict.items():\n",
    "        ngram_dict[k] = v / total_frequency\n",
    "    return ngram_dict\n",
    "\n",
    "unigram_set = getNgramSet(training_set, 1)\n",
    "bigram_set = getNgramSet(training_set, 2)\n",
    "trigram_set = getNgramSet(training_set, 3)\n",
    "fourgram_set = getNgramSet(training_set, 4)\n",
    "fivegram_set = getNgramSet(training_set, 5)\n",
    "\n",
    "ngram_set_dict = {'1': unigram_set, '2': bigram_set, '3': trigram_set, '4': fourgram_set, '5': fivegram_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the lambda for each ngram model\n",
    "def getLambda(ngram_list):\n",
    "    dict = {}\n",
    "    sum = 0\n",
    "    for i in ngram_list:\n",
    "        sum = len(i) + sum\n",
    "        \n",
    "    for i in ngram_list:\n",
    "        dict[len(i)] = len(i)/sum\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main guessing function\n",
    "def ngramAi(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    function for ngram AI play\n",
    "    \"\"\"\n",
    "    # get n for gram model\n",
    "    n = kwargs['ngram']\n",
    "    # add start symbols\n",
    "    for i in range(n-1):\n",
    "        mask.insert(0, '$')\n",
    "        \n",
    "    # get possible characters\n",
    "    characters = list(string.ascii_lowercase)\n",
    "    guessed = list(guessed)\n",
    "    for i in guessed:\n",
    "        characters.remove(i)\n",
    "    \n",
    "    ngram_model_list = []  \n",
    "    # find the ngram pattern in the mask\n",
    "    for i in range((n-1),len(mask)):\n",
    "        previous_words = mask[i-(n-1):i+1]\n",
    "        if previous_words[-1].isalpha():\n",
    "            continue\n",
    "        model = ['_']\n",
    "        flag = 0\n",
    "        for i, v in enumerate(reversed(previous_words)):\n",
    "            if v == '_':\n",
    "                flag += 1\n",
    "                if flag == 2:\n",
    "                    ngram_model_list.append(model)\n",
    "                    break\n",
    "                continue\n",
    "            elif v != '_' and i != (len(previous_words)-1):\n",
    "                model.insert(0, v)\n",
    "            elif v != '_' and i == (len(previous_words)-1):\n",
    "                model.insert(0, v)\n",
    "                ngram_model_list.append(model)\n",
    "            else:\n",
    "                ngram_model_list.append(model)\n",
    "                break\n",
    "#     print(mask)\n",
    "#     print(ngram_model_list)\n",
    "    parameter_set = getLambda(ngram_model_list)\n",
    "    character_possibility = {}\n",
    "    \n",
    "    for i in characters:\n",
    "        for j in ngram_model_list:\n",
    "            gram = ''.join(j[:-1]) + i\n",
    "            ngram_set = ngram_set_dict[str(len(j))]\n",
    "            parameter_lambda = parameter_set[len(j)]\n",
    "            character_possibility[i] = character_possibility.get(i, 0) + (parameter_lambda*ngram_set.get(gram, 0))\n",
    "        \n",
    "    character_possibility = sorted(character_possibility.items(), key=lambda x:x[1], reverse=True)\n",
    "    # remove start symbols\n",
    "    for i in range(n-1):\n",
    "        mask.remove('$')\n",
    "    return character_possibility[0][0]\n",
    "\n",
    "\n",
    "# kwargs = {'ngram': 4}\n",
    "# fault = hangman('hello', ngramAi, 26, True, **kwargs)\n",
    "# print(fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average guessing number of tirgram model  is 8.323\n",
      "The average guessing number of fourgram model  is 8.102\n",
      "The average guessing number of fivegram model  is 8.255\n",
      "The improvement of the guessing number is obvious over the unigram.\n"
     ]
    }
   ],
   "source": [
    "# trigram modle\n",
    "kwargs = {'ngram': 3}\n",
    "mistakes_guess = [hangman(i, ngramAi, 26, False, **kwargs) for i in test_set]\n",
    "ngram = np.average(mistakes_guess)\n",
    "print('The average guessing number of tirgram model  is', ngram)\n",
    "\n",
    "# fourgram modle\n",
    "kwargs = {'ngram': 4}\n",
    "mistakes_guess = [hangman(i, ngramAi, 26, False, **kwargs) for i in test_set]\n",
    "ngram = np.average(mistakes_guess)\n",
    "print('The average guessing number of fourgram model  is', ngram)\n",
    "\n",
    "# fivegram modle\n",
    "kwargs = {'ngram': 5}\n",
    "mistakes_guess = [hangman(i, ngramAi, 26, False, **kwargs) for i in test_set]\n",
    "ngram = np.average(mistakes_guess)\n",
    "print('The average guessing number of fivegram model  is', ngram)\n",
    "\n",
    "print('The improvement of the guessing number is obvious over the unigram.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
